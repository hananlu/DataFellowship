{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqx-SQHFHqeX"
   },
   "source": [
    "# **Data Ingestion, Airflow**\n",
    "Created by [Bagas Prakasa](https://www.linkedin.com/in/bagasprakasa/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3tkRWbaYWvS"
   },
   "source": [
    "## Prequist:\n",
    "1. Python 3.6 or later\n",
    "2. Docker-CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iVhqj_OLpWJ"
   },
   "source": [
    "### **What is Data Ingestion ?**\n",
    "Data ingestion adalah proses pemindahan data dari satu atau beberapa sumber ke suatu penyimpanan. Data tersebut nantinya akan disimpan dan dianalisis lebih lanjut.\n",
    "\n",
    "### Perbedaan data ingestion dan data integration\n",
    "Data integration selangkah lebih rumit dari data ingestion. Hal itu disebabkan dalam proses data ingestion kumpulan data hanya dipindahkan ke lokasi baru. Namun, dalam data integration, kumpulan data tersebut akan dipastikan kesesuaiannya meskipun dari sumber yang berbeda. Dengan begitu, proses menganalisis data bisa dilakukan lebih mudah dan akurat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjSPkEoPRGyN"
   },
   "source": [
    "### **3 Types of Data Ingestion**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1fx_6Fe4USuDUnTemBCa-9_3ZoAqbmO3Z)\n",
    "\n",
    "1. **Real-time**\n",
    "\n",
    "Mengumpulkan dan mentransfer data dari sistem secara real-time menggunakan change data capture (CDC). Nantinya, CDC akan terus memantau proses transfer data tanpa mengganggu beban kerja database. Biasanya, perusahaan yang menerapkan jenis ini adalah yang membutuhkan transfer data secara cepat dan real-time. Contohnya adalah perusahaan untuk perdagangan pasar saham\n",
    "\n",
    "2. **Batch-based**\n",
    "\n",
    "Proses mengumpulkan dan mentransfer data dalam sebuah kumpulan sesuai dengan interval yang telah terjadwal sebelumnya. Sehingga, pengumpulan data bisa berdasarkan jadwal, peristiwa, atau urutan yang telah disesuaikan. Jenis ini berguna ketika perusahaan perlu mengumpulkan data tertentu setiap hari. Serta tidak memerlukan data untuk mengambil keputusan sewaktu-waktu. \n",
    "\n",
    "3. **Lambda architecture-based**\n",
    "\n",
    "Kombinasi dari dua jenis sebelumnya (real-time dan batch-based). Umumnya, lambda architecture-based terdiri dari proses pengumpulan, penyajian, dan lapisan kecepatan. Proses pengumpulan dan penyajian melakukan pengindeksan data. Kemudian, proses lapisan kecepatan akan secara instan mengindeks data yang belum terambil dari proses pengindeksan yang pertama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU4sdHO-h3E7"
   },
   "source": [
    "### **Why We need Airflow ?**\n",
    "#### Typical Usecase\n",
    "Let's discover a typical use case that we will be able to solved with airflow.\n",
    "\n",
    "For example, we have Data Pipeline to trigger everyday at 00:10 AM below.\n",
    "![](https://drive.google.com/uc?export=view&id=1xcxsy4VO_fEh9w0I1ZYWXq9elyznolBY)\n",
    "\n",
    "- We might have the following data pipeline with three tasks: downloading data,processing data and storing data, all of them being executed sequentially.\n",
    "- All of those tasks might interact with other tools as well. For example, the data might request an API. Then processing data will interact with Spark Job and finally, storing data to do inserts and updates to a database.\n",
    "- Besides we have to execute those tasks, we also have to make sure that the external tools such as the API or the database are available in order to be sure that the data pipeline will succeed.\n",
    "\n",
    "The problems that could be happen are?\n",
    "- The API could be not available (Diagram 2)\n",
    "- The Spark job failed because out of memory (Diagram 2)\n",
    "- The database is not available and so we can't doing inserts or updates (Diagram 2)\n",
    "- And If we have hundreds of different data pipelines to manage at the same time, it would be really hard to manage. (Diagram 3)\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1hhiI1PkCP_DYmEsaz1cyyUOszD3wsTpi)\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1IinleZnAdh8lFT1wc5OacNWmtujJJbHN)\n",
    "\n",
    "So, the reason why we need airflow are: \n",
    "- We will be able to manage our data pipelines and execute our tasks in a very reliable way.\n",
    "- We will be able to monitor and retry our tasks automaticallyif there is any failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phj-phjOdjmA"
   },
   "source": [
    "### What is Airflow ?\n",
    "\n",
    "Airflow is an open source platform to programmatically author, schedule and monitor workflows.\n",
    "\n",
    "Another definition, Airflow is an orchestrator allowing you to execute your tasks at the right time, in the right way and in the right order.\n",
    "\n",
    "#### Benefit of Airflow\n",
    "![](https://drive.google.com/uc?export=view&id=15uho4htmdaYOObT-ODEyVwbHiAjqvFeU)\n",
    "\n",
    "- Dynamic : Our data pipeline are dynamic, everything that we can do in Python, we can do it in our data pipelines. So the possibilities are quite limitless at the end and this is truly powerful.\n",
    "- Scalability : We can execute as many tasks as we want in parallel. Obviously, it depends on our architecture, on our resources. We can execute using Local, Celery or Kubernetes Executor.\n",
    "- User Interface (UI) : We will be able to monitor our data problems from it, retry our tasks, interact a lot from the user interface of airflow with our data pipelines.\n",
    "- Extensibility: That means if there is a new tool and we want to interact with that new tool, we don't have to wait for airflow to be upgraded. We can create our own plugin added to our application.\n",
    "\n",
    "#### Airflow Core Components\n",
    "![](https://drive.google.com/uc?export=view&id=1GnlZXhwpbCaS4Dp-ctUbNhOPAgdB67VD)\n",
    "\n",
    "- Web Server : A Flask server with Gunicon serving the user interface.\n",
    "- Scheduler : A daemon in charge of scheduling tasks and data pipelines.\n",
    "- Metastore : A database where all the metadata related to Airflow itself, data pipeline, tasks and so on will be stored. Usually we will use PostgreSQL as it is the recommended database to use with airflow. But we can use MySQL or Oracle as long as the database is compatible with SQLAlchemy.\n",
    "- Executor : Defines how our tasks will be executed, either with Kubernetes, Celery or local executor.\n",
    "- Worker : The process where the task is executed. The difference between executor and worker is the executor defines how the task should be executed, whereas the worker is actually the process executing the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njmYSOllluHs"
   },
   "source": [
    "### What Airflow is not ?\n",
    "![](https://drive.google.com/uc?export=view&id=1hLFypH8mmkfh-XUQWQ3cjjc8-gaFHMOD)\n",
    "\n",
    "Airflow is not a data processing framework, like Apache Spark. If we have to process terabytes of data, we should not do that in airflow. Because it's definitely not optimized to do this. So if we want to that, we have to use  an operator to send the spark job where the terabytes of data will be processed\n",
    "inside Spark and not inside airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzKYUoy8IPY3"
   },
   "source": [
    "### Bagaimana Cara Kerja Airflow?\n",
    "\n",
    "#### One Node Architecture\n",
    "![](https://drive.google.com/uc?export=view&id=1AdmvD4_bj40E3upc_QCjV-GjC3WegFxF)\n",
    "\n",
    "\n",
    "Komponen yang ada dalam One Node Architecture Airflow antara lain Webserver, Scheduler, Metastore dan Executor yang berjalan di dalam satu mesin.\n",
    " \n",
    "Cara kerja Arsitektur 1 Node menjalankan sebuah task antara lain:\n",
    "1. Webserver akan mengambil metadata yang berasal dari Metastore.\n",
    "2. Scheduler berkomunikasi dengan Metastore dan Executor untuk menjadwalkan task yang perlu dijalankan.\n",
    "3. Executor yang sudah diberi tahu oleh Scheduler terkait task yang akan di trigger tersebut selanjutnya akan melakukan *push* task ke Queue. \n",
    "4. Ketika task sudah dalam Queue, Queue akan menentukan dimana task tersebut akan di eksekusi. Karena Node yang tersedia hanya 1, task tersebut dieksekusi juga dalam Node tersebut.\n",
    "\n",
    "Arsitektur ini sangat bagus jika digunakan untuk melakukan hal-hal yang bertujuan untuk experimental saja, bukan untuk production.\n",
    "\n",
    "Jika kita ingin menjalankan task di Airflow untuk tujuan production, kita bisa menggunakan Multi Nodes Architecture (Celery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-2Pi5Ww4W0q"
   },
   "source": [
    "#### Multi Nodes Architecture (Celery)\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Kt4_kDkWela737dltOGkw-sb-ZqHgHgs)\n",
    "Dalam Arsitektur Celery, komponen-komponen yang membentuk Airflow terpisah menjadi beberapa Node.\n",
    "\n",
    "Pada Node 1, terdapat Web server, scheduler dan Executor. Dalam Node 2 terdapat Metastore dan Queue serta ada 3 Worker Node terpisah. Perbedaan antara Arsitektur 1 Node dengan Multiple Node terdapat pada bagian Queue yang terpisah dengan Executor serta adanya Node khusus untuk Worker.\n",
    "\n",
    "Cara kerja Arsitektur Celery menjalankan sebuah task antara lain:\n",
    "1. Webserver akan mengambil metadata yang berasal dari Metastore.\n",
    "2. Scheduler berkomunikasi dengan Metastore dan Executor untuk menjadwalkan task yang perlu dijalankan.\n",
    "3. Executor yang sudah diberi tahu oleh Scheduler terkait task yang akan di trigger tersebut selanjutnya akan melakukan *push* task ke Queue. \n",
    "4. Ketika task sudah dalam Queue, Queue akan menentukan dimana task tersebut akan di eksekusi tergantung Airflow Worker yang sedang idle. \n",
    "5. Airflow yang idle akan mengambil task tersebut untuk dieksekusi di dalam mesin Worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gywad8MsMb9p"
   },
   "source": [
    "### Apa itu DAG ?\n",
    "\n",
    "Sebuah DAG (Directed Acyclic Graph) adalah data pipeline yang ada dalam Airflow, DAG bertugas mengumpulkan Task bersamaan, mengorganisirnya dengan dependencies and relationships untuk menentukan bagaimana Task-task tersebut seharusnya berjalan.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Asz-yz7g4ZzmbFDfo8Chgm_oAr5dKCUS)\n",
    "\n",
    "- Directed : ada arah panahnya. Misal, A dulu baru B.\n",
    "- Acyclic : tidak boleh memutar kembali ke job sebelumnya.\n",
    "- Graph : Sumber dan targetnya bisa banyak.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1i8-I7BEUuDt5l2UI19j10ZQpbyjxCvnA)\n",
    "\n",
    "\n",
    "Gambar diatas menjelaskan bagaimana task di dalam DAG di eksekusi satu per satu sesuai dengan urutan dependency. Dimulai dari T1, kemudian bercabang ke T2 dan T3, kemudian T4 dan terakhir bercabang ke T5 dan T6. DAG yang benar pasti memiliki urutan task yang searah, tidak pernah berulang. Jika berulang, maka DAG akan mengalami error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sYt7Z8HIv-O"
   },
   "source": [
    "### Peran DAG di dalam Airflow\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=14lu61QGeOkhKSPl50S6jtWqnQlgmu1LF)\n",
    "\n",
    "Dalam ilustrasi ini, kita menggunakan single node. Berikut penjelasan dari diagram diatas:\n",
    "1. Pertama, kita membuat dag file (python file) yaitu dag.py dan meletakkannya di dalam folder dags Airflow.\n",
    "2. Webserver dan Scheduler akan melakukan parsing folder dags tersebut untuk mengecek jika ada file dag baru.\n",
    "3. Ketika new dag terdeteksi dan tertrigger, DagRun object akan langsung terbentuk dan Scheduler mengupdatenya ke Metastore. DagRun adalah instance dari sebuah dag. \n",
    "4. DagRun terbentuk dan memiliki status running. Selanjutnya, Task pertama yang ada di dalam data pipeline kita terschedule. \n",
    "5. Ketika Task tersebut ready untuk di trigger, maka TaskInstance object akan terbentuk.\n",
    "6. Selanjutnya, Scheduler akan mengirimkan TaskInstance tersebut ke Executor.\n",
    "7. TaskInstance running.\n",
    "8. Executor mengupdate status dari TaskInstance ke Metastore.\n",
    "9. Ketika TaskInstance success dan completed, Scheduler akan mengecek status TaskInstance tersebut dan akan mengupdatenya di Metastore.\n",
    "10. Jika semua TaskInstance completed dan status dari DagRun completed juga, Webserver akan melakukan update status dag tersebut menjadi success completed di Airflow UI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U52RrnFT7_P"
   },
   "source": [
    "## **OLTP VS OLAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD_KA6bzT5LA"
   },
   "source": [
    "### What is OLTP?\n",
    "\n",
    "An OLTP system **captures and maintains transaction data in a database**. Each transaction involves individual database records made up of multiple fields or columns. Examples include banking and credit card activity or retail checkout scanning.\n",
    "\n",
    "In OLTP, the emphasis is on fast processing, because OLTP databases are read, written, and updated frequently. If a transaction fails, built-in system logic ensures data integrity.\n",
    "\n",
    "### What is OLAP?\n",
    "OLAP applies complex queries to large amounts of historical data, aggregated from OLTP databases and other sources, for **data mining, analytics, and business intelligence projects**. \n",
    "\n",
    "In OLAP, the emphasis is on response time to these complex queries. Each query involves one or more columns of data aggregated from many rows. Examples include year-over-year financial performance or marketing lead generation trends. OLAP databases and data warehouses give analysts and decision-makers the ability to use custom reporting tools to turn data into information. Query failure in OLAP does not interrupt or delay transaction processing for customers, but it can delay or impact the accuracy of business intelligence insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl0A0MXFVKbc"
   },
   "source": [
    "### **Perbedaan OLAP dan OLTP**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Ravkrc2jzHx4fW1uDiITWaG_2sukNAyL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
